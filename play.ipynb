{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4deee8d6",
   "metadata": {},
   "source": [
    "## Wordy Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a11c6e",
   "metadata": {},
   "source": [
    "A Byte Pair Encoding (BPE) tokenizer splits text into subword units based on the most frequent character pairs in a corpus,\\\n",
    "allowing it to balance vocabulary size and represent rare words efficiently. It starts with individual bytes or characters \\\n",
    "and repeatedly merges the most common adjacent pairs into new tokens until a fixed vocabulary size is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d796c8",
   "metadata": {},
   "source": [
    "Modern BPE tokenizers used for training and inference in large language models typically apply regex-based pretokenization. \\\n",
    "This step splits text into linguistically or visually meaningful chunks (like words or punctuation groups), preventing merges \\\n",
    "across token boundaries that could produce spurious or misleading tokens (e.g., treating \"dog\" and \"dog!\" as entirely different tokens).\n",
    "\n",
    "Regex pretokenization also enables more efficient frequency counting: if a word like â€œtextâ€ appears 10 times, we can increment pair counts (like 't','e') by 10 directly.\\\n",
    "When a merge occurs (e.g., 't','e','x','t' â†’ 'te','x','t'), only the keys in the frequency dictionary need to be updated â€” the total count remains the same, \\\n",
    "which simplifies and speeds up the BPE merge step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a1c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e52abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here: https://github.com/openai/tiktoken/pull/234/files\n",
    "# Using this pattern re.finditer will produce one pretoken per group\n",
    "GPT2_SPLIT_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "pattern = re.compile(GPT2_SPLIT_PAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4b382",
   "metadata": {},
   "source": [
    "## Pieces of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12122c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix of languages + emoji\n",
    "text = \"ÐŸÑ€Ð¸Ð²ÐµÑ‚, world! ðŸ˜„ Let's go, ä»Šæ—¥ã¯.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1fd8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit larger piece of text\n",
    "text = \"\"\"Alice looked at the glowing sign: â€œÐ”Ð¾Ð±Ñ€Ð¾ Ð¿Ð¾Ð¶Ð°Ð»Ð¾Ð²Ð°Ñ‚ÑŒ!â€ â€” it blinked beneath a line of Chinese characters: æ¬¢è¿Žå…‰ä¸´.\n",
    "\n",
    "She typed quickly: `hello_ä¸–ç•Œ123! :)` â€” mixing English, symbols, digits, and emojis into her message.  \n",
    "The response came instantly: \"ÐŸÑ€Ð¸Ð½ÑÑ‚Ð¾. âœ…\"  \n",
    "She smiled, whispered Â«è¡Œå§Â», and pressed Send.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce1327ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another one from cs336 assignment\n",
    "text = \"\"\" low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa92645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' lower',\n",
       " ' lower',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "205ad910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as described above this dictionary will keep pretoken counts\n",
    "# tuple(bytes('ÐŸÑ€Ð¸Ð²ÐµÑ‚', encoding='utf-8')) -> integer represented byte sequence for convenience\n",
    "pretokens = dict()\n",
    "for mt in pattern.finditer(text):\n",
    "  pt = mt.group() # -> str; match will have one pretoken per group\n",
    "  pt = tuple(pt.encode('utf-8'))\n",
    "  pretokens[pt] = pretokens.get(pt, 0) + 1\n",
    "\n",
    "next_ix = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "adba4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = [int.to_bytes(i) for i in range(256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f8d5ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'ne', b'west'] : 6\n"
     ]
    }
   ],
   "source": [
    "for pt, cnt in pretokens.items():\n",
    "  print([merges[i] for i in pt], ':', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4c774a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 119, 105, 100, 257): 3, (260,): 5, (260, 101, 114): 2, (32, 262, 261): 6}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4b00b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to iterate through pretokens to find pair frequencies\n",
    "pair_counts = dict()\n",
    "for pt, cnt in pretokens.items():\n",
    "  for p in zip(pt, pt[1:]):\n",
    "    pair_counts[p] = pair_counts.get(p, 0) + cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "869d2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' ' + b'w' : 3\n",
      "b'w' + b'i' : 3\n",
      "b'i' + b'd' : 3\n",
      "b'd' + b'est' : 3\n",
      "b' low' + b'e' : 2\n",
      "b'e' + b'r' : 2\n",
      "b' ' + b'ne' : 6\n",
      "b'ne' + b'west' : 6\n"
     ]
    }
   ],
   "source": [
    "for p, cnt in pair_counts.items():\n",
    "  print(merges[p[0]], '+' ,merges[p[1]], \":\", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4eaa79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most frequent pair, ties resolved in lexicographical order\n",
    "top_pair, top_cnt = max(pair_counts.items(), key=lambda it: [it[1], it[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "24c6de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 261)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cb8c25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ne' , b'west' -> 6\n"
     ]
    }
   ],
   "source": [
    "print(merges[top_pair[0]], ',', merges[top_pair[1]], '->', top_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d08634fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge `pair` to become `new_ix` if it's in the `seq`\n",
    "def merge(seq, pair, new_ix):\n",
    "  new_seq = []\n",
    "  i = 0\n",
    "  while i < len(seq):\n",
    "    # check in range and if match\n",
    "    if i+1 < len(seq) and (seq[i], seq[i+1]) == pair:\n",
    "      new_seq.append(new_ix)\n",
    "      i += 2 # correct step\n",
    "    else:\n",
    "      new_seq.append(seq[i]) # only current position\n",
    "      i += 1\n",
    "  return tuple(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3c17b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each merge introduces a new token (pair â†’ new token) that wasnâ€™t in the vocabulary before\n",
    "# Pretoken keys are sequences of current tokens.\n",
    "# Until you merge ('t', 'e') into 'te', there's no way 'te' appears as a unit inside any key\n",
    "# Only keys that contain the exact pair ('t', 'e') in adjacent positions will be modified.\n",
    "# The output of merge() depends deterministically on the input key.\n",
    "# Therefore, at most one original key can produce any given new_pt in the merge step.\n",
    "for pt in list(pretokens): # static copy of keys (prevents RuntimeError if we iterate original dict)\n",
    "  new_pt = merge(pt, top_pair, next_ix)\n",
    "  if new_pt != pt: # update only if we merged new index\n",
    "    # even though we proved it can't happen (see above), we want this assertions and perhaps test against it\n",
    "    # so we are sure not to mess up with implementation\n",
    "    assert new_pt not in pretokens, f\"Collision: {new_pt} already in pretokens\"\n",
    "    pretokens[new_pt] = pretokens.pop(pt) #  safe from key collisions under the BPE merge assumptions (see above)\n",
    "  \n",
    "# update merges\n",
    "merges.append(merges[top_pair[0]] + merges[top_pair[1]])\n",
    "next_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "aa56649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 -> b'st'\n",
      "257 -> b'est'\n",
      "258 -> b'ow'\n",
      "259 -> b'low'\n",
      "260 -> b' low'\n",
      "261 -> b'west'\n",
      "262 -> b'ne'\n",
      "263 -> b'newest'\n"
     ]
    }
   ],
   "source": [
    "# we can take a look into newly formed tokens\n",
    "for i, bp in enumerate(merges[256:], 256):\n",
    "  print(i, '->', bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbc59b",
   "metadata": {},
   "source": [
    "## Let's Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge `pair` to become `new_ix` if it's in the `seq`\n",
    "def merge(seq, pair, new_ix):\n",
    "  new_seq = []\n",
    "  i = 0\n",
    "  while i < len(seq):\n",
    "    # check in range and if match\n",
    "    if i+1 < len(seq) and (seq[i], seq[i+1]) == pair:\n",
    "      new_seq.append(new_ix)\n",
    "      i += 2 # correct step\n",
    "    else:\n",
    "      new_seq.append(seq[i]) # only current position\n",
    "      i += 1\n",
    "  return tuple(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5fa56594",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = [int.to_bytes(i) for i in range(256)]\n",
    "\n",
    "# as described above this dictionary will keep pretoken counts\n",
    "# tuple(bytes('ÐŸÑ€Ð¸Ð²ÐµÑ‚', encoding='utf-8')) -> integer represented byte sequence for convenience\n",
    "pretokens = dict()\n",
    "for mt in pattern.finditer(text):\n",
    "  pt = mt.group() # -> str; match will have one pretoken per group\n",
    "  pt = tuple(pt.encode('utf-8'))\n",
    "  pretokens[pt] = pretokens.get(pt, 0) + 1\n",
    "\n",
    "next_ix = 256\n",
    "num_merges = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6c0f98e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'e', b's', b't'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'e', b's', b't'] : 6\n",
      "top pair b's' , b't' -> 9\n",
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'e', b'st'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'e', b'st'] : 6\n",
      "top pair b'e' , b'st' -> 9\n",
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "top pair b'o' , b'w' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' ', b'l', b'ow'] : 5\n",
      "[b' ', b'l', b'ow', b'e', b'r'] : 2\n",
      "top pair b'l' , b'ow' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' ', b'low'] : 5\n",
      "[b' ', b'low', b'e', b'r'] : 2\n",
      "top pair b' ' , b'low' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "top pair b'w' , b'est' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'n', b'e', b'west'] : 6\n",
      "top pair b'n' , b'e' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'ne', b'west'] : 6\n",
      "top pair b'ne' , b'west' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'newest'] : 6\n",
      "top pair b' ' , b'newest' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' newest'] : 6\n",
      "top pair b'w' , b'i' -> 3\n",
      "===================================\n",
      "256 -> b'st'\n",
      "257 -> b'est'\n",
      "258 -> b'ow'\n",
      "259 -> b'low'\n",
      "260 -> b' low'\n",
      "261 -> b'west'\n",
      "262 -> b'ne'\n",
      "263 -> b'newest'\n",
      "264 -> b' newest'\n",
      "265 -> b'wi'\n"
     ]
    }
   ],
   "source": [
    "sep = \"===================================\"\n",
    "for _ in range(num_merges):\n",
    "  print(sep)\n",
    "  # show pretokens\n",
    "  for pt, cnt in pretokens.items():\n",
    "    print([merges[i] for i in pt], ':', cnt)\n",
    "  \n",
    "  # we need to iterate through pretokens to find pair frequencies\n",
    "  pair_counts = dict()\n",
    "  for pt, cnt in pretokens.items():\n",
    "    for p in zip(pt, pt[1:]):\n",
    "      pair_counts[p] = pair_counts.get(p, 0) + cnt\n",
    "  # find most frequent pair, ties resolved in lexicographical order\n",
    "  top_pair, top_cnt = max(pair_counts.items(), key=lambda it: [it[1], it[0]])\n",
    "  print(\"top pair\", merges[top_pair[0]], ',', merges[top_pair[1]], '->', top_cnt)\n",
    "  \n",
    "  # merge\n",
    "  for pt in list(pretokens): # static copy of keys (prevents RuntimeError if we iterate original dict)\n",
    "    new_pt = merge(pt, top_pair, next_ix)\n",
    "    if new_pt != pt: # update only if we merged new index\n",
    "      # even though we proved it can't happen (see above), we want this assertions and perhaps test against it\n",
    "      # so we are sure not to mess up with implementation\n",
    "      assert new_pt not in pretokens, f\"Collision: {new_pt} already in pretokens\"\n",
    "      pretokens[new_pt] = pretokens.pop(pt) #  safe from key collisions under the BPE merge assumptions (see above)\n",
    "  \n",
    "  # update merges\n",
    "  merges.append(merges[top_pair[0]] + merges[top_pair[1]])\n",
    "  next_ix += 1\n",
    "\n",
    "print(sep)\n",
    "# we can take a look into newly formed tokens\n",
    "for i, bp in enumerate(merges[256:], 256):\n",
    "  print(i, '->', bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f0af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
