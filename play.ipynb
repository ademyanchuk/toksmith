{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4deee8d6",
   "metadata": {},
   "source": [
    "## Wordy Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a11c6e",
   "metadata": {},
   "source": [
    "A Byte Pair Encoding (BPE) tokenizer splits text into subword units based on the most frequent character pairs in a corpus,\\\n",
    "allowing it to balance vocabulary size and represent rare words efficiently. It starts with individual bytes or characters \\\n",
    "and repeatedly merges the most common adjacent pairs into new tokens until a fixed vocabulary size is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d796c8",
   "metadata": {},
   "source": [
    "Modern BPE tokenizers used for training and inference in large language models typically apply regex-based pretokenization. \\\n",
    "This step splits text into linguistically or visually meaningful chunks (like words or punctuation groups), preventing merges \\\n",
    "across token boundaries that could produce spurious or misleading tokens (e.g., treating \"dog\" and \"dog!\" as entirely different tokens).\n",
    "\n",
    "Regex pretokenization also enables more efficient frequency counting: if a word like â€œtextâ€ appears 10 times, we can increment pair counts (like 't','e') by 10 directly.\\\n",
    "When a merge occurs (e.g., 't','e','x','t' â†’ 'te','x','t'), only the keys in the frequency dictionary need to be updated â€” the total count remains the same, \\\n",
    "which simplifies and speeds up the BPE merge step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a1c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e52abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here: https://github.com/openai/tiktoken/pull/234/files\n",
    "# Using this pattern re.finditer will produce one pretoken per group\n",
    "GPT2_SPLIT_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "pattern = re.compile(GPT2_SPLIT_PAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4b382",
   "metadata": {},
   "source": [
    "## Pieces of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12122c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix of languages + emoji\n",
    "text = \"ÐŸÑ€Ð¸Ð²ÐµÑ‚, world! ðŸ˜„ Let's go, ä»Šæ—¥ã¯.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1fd8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit larger piece of text\n",
    "text = \"\"\"Alice looked at the glowing sign: â€œÐ”Ð¾Ð±Ñ€Ð¾ Ð¿Ð¾Ð¶Ð°Ð»Ð¾Ð²Ð°Ñ‚ÑŒ!â€ â€” it blinked beneath a line of Chinese characters: æ¬¢è¿Žå…‰ä¸´.\n",
    "\n",
    "She typed quickly: `hello_ä¸–ç•Œ123! :)` â€” mixing English, symbols, digits, and emojis into her message.  \n",
    "The response came instantly: \"ÐŸÑ€Ð¸Ð½ÑÑ‚Ð¾. âœ…\"  \n",
    "She smiled, whispered Â«è¡Œå§Â», and pressed Send.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce1327ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another one from cs336 assignment\n",
    "text = \"\"\" low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa92645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' lower',\n",
       " ' lower',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "205ad910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as described above this dictionary will keep pretoken counts\n",
    "# tuple(bytes('ÐŸÑ€Ð¸Ð²ÐµÑ‚', encoding='utf-8')) -> integer represented byte sequence for convenience\n",
    "pretokens = dict()\n",
    "for mt in pattern.finditer(text):\n",
    "  pt = mt.group() # -> str; match will have one pretoken per group\n",
    "  pt = tuple(pt.encode('utf-8'))\n",
    "  pretokens[pt] = pretokens.get(pt, 0) + 1\n",
    "\n",
    "next_ix = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "adba4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = [int.to_bytes(i) for i in range(256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f8d5ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'ne', b'west'] : 6\n"
     ]
    }
   ],
   "source": [
    "for pt, cnt in pretokens.items():\n",
    "  print([merges[i] for i in pt], ':', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4c774a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 119, 105, 100, 257): 3, (260,): 5, (260, 101, 114): 2, (32, 262, 261): 6}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4b00b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to iterate through pretokens to find pair frequencies\n",
    "pair_counts = dict()\n",
    "for pt, cnt in pretokens.items():\n",
    "  for p in zip(pt, pt[1:]):\n",
    "    pair_counts[p] = pair_counts.get(p, 0) + cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "869d2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' ' + b'w' : 3\n",
      "b'w' + b'i' : 3\n",
      "b'i' + b'd' : 3\n",
      "b'd' + b'est' : 3\n",
      "b' low' + b'e' : 2\n",
      "b'e' + b'r' : 2\n",
      "b' ' + b'ne' : 6\n",
      "b'ne' + b'west' : 6\n"
     ]
    }
   ],
   "source": [
    "for p, cnt in pair_counts.items():\n",
    "  print(merges[p[0]], '+' ,merges[p[1]], \":\", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4eaa79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most frequent pair, ties resolved in lexicographical order\n",
    "top_pair, top_cnt = max(pair_counts.items(), key=lambda it: [it[1], it[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "24c6de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 261)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cb8c25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ne' , b'west' -> 6\n"
     ]
    }
   ],
   "source": [
    "print(merges[top_pair[0]], ',', merges[top_pair[1]], '->', top_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d08634fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge `pair` to become `new_ix` if it's in the `seq`\n",
    "def merge(seq, pair, new_ix):\n",
    "  new_seq = []\n",
    "  i = 0\n",
    "  while i < len(seq):\n",
    "    # check in range and if match\n",
    "    if i+1 < len(seq) and (seq[i], seq[i+1]) == pair:\n",
    "      new_seq.append(new_ix)\n",
    "      i += 2 # correct step\n",
    "    else:\n",
    "      new_seq.append(seq[i]) # only current position\n",
    "      i += 1\n",
    "  return tuple(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3c17b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each merge introduces a new token (pair â†’ new token) that wasnâ€™t in the vocabulary before\n",
    "# Pretoken keys are sequences of current tokens.\n",
    "# Until you merge ('t', 'e') into 'te', there's no way 'te' appears as a unit inside any key\n",
    "# Only keys that contain the exact pair ('t', 'e') in adjacent positions will be modified.\n",
    "# The output of merge() depends deterministically on the input key.\n",
    "# Therefore, at most one original key can produce any given new_pt in the merge step.\n",
    "for pt in list(pretokens): # static copy of keys (prevents RuntimeError if we iterate original dict)\n",
    "  new_pt = merge(pt, top_pair, next_ix)\n",
    "  if new_pt != pt: # update only if we merged new index\n",
    "    # even though we proved it can't happen (see above), we want this assertions and perhaps test against it\n",
    "    # so we are sure not to mess up with implementation\n",
    "    assert new_pt not in pretokens, f\"Collision: {new_pt} already in pretokens\"\n",
    "    pretokens[new_pt] = pretokens.pop(pt) #  safe from key collisions under the BPE merge assumptions (see above)\n",
    "  \n",
    "# update merges\n",
    "merges.append(merges[top_pair[0]] + merges[top_pair[1]])\n",
    "next_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "aa56649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 -> b'st'\n",
      "257 -> b'est'\n",
      "258 -> b'ow'\n",
      "259 -> b'low'\n",
      "260 -> b' low'\n",
      "261 -> b'west'\n",
      "262 -> b'ne'\n",
      "263 -> b'newest'\n"
     ]
    }
   ],
   "source": [
    "# we can take a look into newly formed tokens\n",
    "for i, bp in enumerate(merges[256:], 256):\n",
    "  print(i, '->', bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbc59b",
   "metadata": {},
   "source": [
    "## Let's Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge `pair` to become `new_ix` if it's in the `seq`\n",
    "def merge(seq, pair, new_ix):\n",
    "  new_seq = []\n",
    "  i = 0\n",
    "  while i < len(seq):\n",
    "    # check in range and if match\n",
    "    if i+1 < len(seq) and (seq[i], seq[i+1]) == pair:\n",
    "      new_seq.append(new_ix)\n",
    "      i += 2 # correct step\n",
    "    else:\n",
    "      new_seq.append(seq[i]) # only current position\n",
    "      i += 1\n",
    "  return tuple(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5fa56594",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = [int.to_bytes(i) for i in range(256)]\n",
    "\n",
    "# as described above this dictionary will keep pretoken counts\n",
    "# tuple(bytes('ÐŸÑ€Ð¸Ð²ÐµÑ‚', encoding='utf-8')) -> integer represented byte sequence for convenience\n",
    "pretokens = dict()\n",
    "for mt in pattern.finditer(text):\n",
    "  pt = mt.group() # -> str; match will have one pretoken per group\n",
    "  pt = tuple(pt.encode('utf-8'))\n",
    "  pretokens[pt] = pretokens.get(pt, 0) + 1\n",
    "\n",
    "next_ix = 256\n",
    "num_merges = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6c0f98e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'e', b's', b't'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'e', b's', b't'] : 6\n",
      "top pair b's' , b't' -> 9\n",
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'e', b'st'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'e', b'st'] : 6\n",
      "top pair b'e' , b'st' -> 9\n",
      "===================================\n",
      "[b' ', b'l', b'o', b'w'] : 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] : 2\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "top pair b'o' , b'w' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' ', b'l', b'ow'] : 5\n",
      "[b' ', b'l', b'ow', b'e', b'r'] : 2\n",
      "top pair b'l' , b'ow' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' ', b'low'] : 5\n",
      "[b' ', b'low', b'e', b'r'] : 2\n",
      "top pair b' ' , b'low' -> 7\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' ', b'n', b'e', b'w', b'est'] : 6\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "top pair b'w' , b'est' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'n', b'e', b'west'] : 6\n",
      "top pair b'n' , b'e' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'ne', b'west'] : 6\n",
      "top pair b'ne' , b'west' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' ', b'newest'] : 6\n",
      "top pair b' ' , b'newest' -> 6\n",
      "===================================\n",
      "[b' ', b'w', b'i', b'd', b'est'] : 3\n",
      "[b' low'] : 5\n",
      "[b' low', b'e', b'r'] : 2\n",
      "[b' newest'] : 6\n",
      "top pair b'w' , b'i' -> 3\n",
      "===================================\n",
      "256 -> b'st'\n",
      "257 -> b'est'\n",
      "258 -> b'ow'\n",
      "259 -> b'low'\n",
      "260 -> b' low'\n",
      "261 -> b'west'\n",
      "262 -> b'ne'\n",
      "263 -> b'newest'\n",
      "264 -> b' newest'\n",
      "265 -> b'wi'\n"
     ]
    }
   ],
   "source": [
    "sep = \"===================================\"\n",
    "for _ in range(num_merges):\n",
    "  print(sep)\n",
    "  # show pretokens\n",
    "  for pt, cnt in pretokens.items():\n",
    "    print([merges[i] for i in pt], ':', cnt)\n",
    "  \n",
    "  # we need to iterate through pretokens to find pair frequencies\n",
    "  pair_counts = dict()\n",
    "  for pt, cnt in pretokens.items():\n",
    "    for p in zip(pt, pt[1:]):\n",
    "      pair_counts[p] = pair_counts.get(p, 0) + cnt\n",
    "  # find most frequent pair, ties resolved in lexicographical order\n",
    "  top_pair, top_cnt = max(pair_counts.items(), key=lambda it: [it[1], it[0]])\n",
    "  print(\"top pair\", merges[top_pair[0]], ',', merges[top_pair[1]], '->', top_cnt)\n",
    "  \n",
    "  # merge\n",
    "  for pt in list(pretokens): # static copy of keys (prevents RuntimeError if we iterate original dict)\n",
    "    new_pt = merge(pt, top_pair, next_ix)\n",
    "    if new_pt != pt: # update only if we merged new index\n",
    "      # even though we proved it can't happen (see above), we want this assertions and perhaps test against it\n",
    "      # so we are sure not to mess up with implementation\n",
    "      assert new_pt not in pretokens, f\"Collision: {new_pt} already in pretokens\"\n",
    "      pretokens[new_pt] = pretokens.pop(pt) #  safe from key collisions under the BPE merge assumptions (see above)\n",
    "  \n",
    "  # update merges\n",
    "  merges.append(merges[top_pair[0]] + merges[top_pair[1]])\n",
    "  next_ix += 1\n",
    "\n",
    "print(sep)\n",
    "# we can take a look into newly formed tokens\n",
    "for i, bp in enumerate(merges[256:], 256):\n",
    "  print(i, '->', bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f0af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\\|endoftext\\|>|<\\|endofline\\|>\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<|endoftext|>\", \"<|endofline|>\"]\n",
    "delim = \"|\".join(map(re.escape, special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bd1a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/TinyStoriesV2-GPT4-valid.txt\", mode='r', encoding=\"utf-8\") as f:\n",
    "  ts_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a3844e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22493387"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64768c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n<|endoftext|>\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red b'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b57cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = re.split(delim, ts_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2db6daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27631"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c89e35f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOnce upon a time, there was a little boy named Tim. Yesterday, he went to a shop with his mom. Tim saw a big, red ball. He wanted it so much. He asked his mom, \"Can I have the ball, please?\" His mom said, \"Yes, you can have it.\"\\nTim was so happy. He played with the ball all day. But then, he kicked the ball too hard. The ball flew away and hit a tree. The tree was hurt. The tree said, \"Ouch! That hurt! Please be careful next time.\"\\nTim felt sorry for the tree. He said, \"I\\'m sorry, tree. I will be more careful.\" He picked up the ball and took it home. Tim\\'s mom ordered a new, soft ball for the tree. The tree was happy, and Tim learned to play more carefully. They all lived happily ever after.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[2343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67e60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\\nSam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\\nThey went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\\n\\n\\n\\nTom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\\n\"Tom, can I have some blocks too?\" Lily asked. She wanted to make a bridge for her cars.\\n\"No, these are mine. Go find your own,\" Tom said. He did not want to share with his sister. He pulled the blocks closer to him.\\nLily felt sad and angry. She did not think Tom was being nice. She looked at his tower and had an idea. She decided to pull one of the blocks at the bottom of the tower.\\nSuddenly, the tower fell down with a loud crash. All the blocks and cars scattered on the floor. Tom and Lily were shocked. They felt the floor shake and heard a rumble. It was an earthquake!\\n\"Mommy! Daddy!\" they cried. They were scared and ran to their parents, who were in the kitchen.\\n\"Are you okay, kids?\" Mommy asked. She hugged them and checked if they were hurt.\\n\"We\\'re okay, Mommy. But our toys are broken,\" Lily said.\\n\"I\\'m sorry, Lily. But toys are not important. You are important. We are safe and together. That\\'s what matters,\" Mommy said.\\nTom felt sorry for what he did. He realized he was selfish and mean to his sister. He saw how scared she was during the earthquake. He wanted to make her happy.\\n\"Lily, I\\'m sorry I did not share with you. You can have all the blocks you want. I love you, sister,\" Tom said.\\nLily smiled and hugged him. She forgave him and thanked him. She loved him too.\\nThey went back to the living room and cleaned up their toys. They decided to build something together. They made a big house with a garden and a fence. They put their cars and dolls inside. They were happy and proud of their work.\\nMommy and Daddy came to see their house. They praised them and gave them a treat. It was a lemon cake. It was sour, but they liked it. They learned that sharing is caring, and that family is sweet.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\".join(stories[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\\nSam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\\nThey went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\\n\\n\\n\\nTom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\\n\"Tom, can I have some blocks too?\" Lily asked. She wanted to make a bridge for her cars.\\n\"No, these are mine. Go find your own,\" Tom said. He did not want to share with his sister. He pulled the blocks closer to him.\\nLily felt sad and angry. She did not think Tom was being nice. She looked at his tower and had an idea. She decided to pull one of the blocks at the bottom of the tower.\\nSuddenly, the tower fell down with a loud crash. All the blocks and cars scattered on the floor. Tom and Lily were shocked. They felt the floor shake and heard a rumble. It was an earthquake!\\n\"Mommy! Daddy!\" they cried. They were scared and ran to their parents, who were in the kitchen.\\n\"Are you okay, kids?\" Mommy asked. She hugged them and checked if they were hurt.\\n\"We\\'re okay, Mommy. But our toys are broken,\" Lily said.\\n\"I\\'m sorry, Lily. But toys are not important. You are important. We are safe and together. That\\'s what matters,\" Mommy said.\\nTom felt sorry for what he did. He realized he was selfish and mean to his sister. He saw how scared she was during the earthquake. He wanted to make her happy.\\n\"Lily, I\\'m sorry I did not share with you. You can have all the blocks you want. I love you, sister,\" Tom said.\\nLily smiled and hugged him. She forgave him and thanked him. She loved him too.\\nThey went back to the living room and cleaned up their toys. They decided to build something together. They made a big house with a garden and a fence. They put their cars and dolls inside. They were happy and proud of their work.\\nMommy and Daddy came to see their house. They praised them and gave them a treat. It was a lemon cake. It was sour, but they liked it. They learned that sharing is caring, and that family is sweet.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\".join(stories[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d47991f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes('<|endoftext|>', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ff2263c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world !'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"Hello<tok>world<tok><eod>!\"\n",
    "special_tokens = [\"<tok>\", \"<eod>\"]\n",
    "delim = \"|\".join(map(re.escape, special_tokens))\n",
    "re.sub(f\"(?:{delim})+\", \" \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8633c6",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795a3e5",
   "metadata": {},
   "source": [
    "1. We can do merge and count updates on only sequences really containing top pair -> need pair_to_sequence mapping Dict[tuple[int, int], Set[tuple[int, ...], ...]]\n",
    "\n",
    "2. We can use heap to keep track of top pairs for O(log n) max pair look up. Real life training can lead to a really large number of unique pairs. Number of possible unique pairs depends on vocab size, which grows every merge iteration (256 -> vocab_size). If vocab_size = n, we have P(n, 2) = n! / (n - 2)! \n",
    "\n",
    "Important note: we shouldn't forget about updating counts for overlapping pairs before and after merge, as well as merged top pair itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9075da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/TinyStoriesV2-GPT4-valid.txt\", mode='r', encoding=\"utf-8\") as f:\n",
    "  ts_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919b3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ts_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd55dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did step-by-step training on this text, so for the first test we should see that optimized\n",
    "# version reproduces naive implementation results.\n",
    "text = \"\"\" low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7108180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 95)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac856ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same pre-token counter\n",
    "from toksmith.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd83fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a890a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures\n",
    "# I am using byte-unit term to underline that tokens are formed as a sequences of units, which correspond to either\n",
    "# one byte or multiple bytes\n",
    "# 1. pretoken_count: Dict[tuple[int, ...], int] = pretokens (as byte-unit sequences) frequency dictionary\n",
    "# 2. pairs_count: Dict[tuple[int, int], int] = count of all sequential pairs of byte-units in the train corpus \n",
    "# 3. pair_to_pretoken: Dict[tuple[int, int], Set[tuple[int, ...], ...]] = map a pair to all pretokens, which have it\n",
    "# this data structure will allow us to only touch relevant pretokens during merge iteration (and not all of them as\n",
    "# in our naive implementation).\n",
    "# 4. max heap on List[tuple[int, tuple[int, int]], ...] = provides a O(log n) access to most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b18fb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76cbbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pretoken_count(text: str) -> dict[tuple[int, ...], int]:\n",
    "    pretokens = dict()\n",
    "    for mt in tok.pattern.finditer(text):\n",
    "      pt = mt.group()  # -> str; match will have one pretoken per group\n",
    "      pt = tuple(pt.encode('utf-8'))\n",
    "      pretokens[pt] = pretokens.get(pt, 0) + 1\n",
    "    return pretokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501eb53",
   "metadata": {},
   "source": [
    "### One Time Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1d51260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab if we need to print\n",
    "vocab = {i : bytes([i]) for i in range(256)}\n",
    "merges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "530653d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pretoken count\n",
    "pretoken_count = _pretoken_count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bd1df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b' ', b'l', b'o', b'w'] -> 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] -> 2\n",
      "[b' ', b'w', b'i', b'd', b'e', b's', b't'] -> 3\n",
      "[b' ', b'n', b'e', b'w', b'e', b's', b't'] -> 6\n"
     ]
    }
   ],
   "source": [
    "for pt, cnt in pretoken_count.items():\n",
    "  print([vocab[unit] for unit in pt], '->', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "878b2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pairs_count and pair_to_pretoken\n",
    "pairs_count = dict()\n",
    "pair_to_pretoken = dict()\n",
    "# basically the same code as in our _pairs_count\n",
    "for pt, cnt in pretoken_count.items():\n",
    "  for pair in zip(pt, pt[1:]):\n",
    "    pairs_count[pair] = pairs_count.get(pair, 0) + cnt\n",
    "    if pair in pair_to_pretoken:\n",
    "      pair_to_pretoken[pair].add(pt)\n",
    "    else:\n",
    "      pair_to_pretoken[pair] = {pt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f0f834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 108) -> ( b' ' , b'l' ) -> 7\n",
      "(108, 111) -> ( b'l' , b'o' ) -> 7\n",
      "(111, 119) -> ( b'o' , b'w' ) -> 7\n",
      "(119, 101) -> ( b'w' , b'e' ) -> 8\n",
      "(101, 114) -> ( b'e' , b'r' ) -> 2\n",
      "(32, 119) -> ( b' ' , b'w' ) -> 3\n",
      "(119, 105) -> ( b'w' , b'i' ) -> 3\n",
      "(105, 100) -> ( b'i' , b'd' ) -> 3\n",
      "(100, 101) -> ( b'd' , b'e' ) -> 3\n",
      "(101, 115) -> ( b'e' , b's' ) -> 9\n",
      "(115, 116) -> ( b's' , b't' ) -> 9\n",
      "(32, 110) -> ( b' ' , b'n' ) -> 6\n",
      "(110, 101) -> ( b'n' , b'e' ) -> 6\n",
      "(101, 119) -> ( b'e' , b'w' ) -> 6\n"
     ]
    }
   ],
   "source": [
    "for pair, cnt in pairs_count.items():\n",
    "  print(pair, '->', '(', vocab[pair[0]], ',', vocab[pair[1]], ')', '->', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13889868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 108): {(32, 108, 111, 119), (32, 108, 111, 119, 101, 114)},\n",
       " (108, 111): {(32, 108, 111, 119), (32, 108, 111, 119, 101, 114)},\n",
       " (111, 119): {(32, 108, 111, 119), (32, 108, 111, 119, 101, 114)},\n",
       " (119, 101): {(32, 108, 111, 119, 101, 114),\n",
       "  (32, 110, 101, 119, 101, 115, 116)},\n",
       " (101, 114): {(32, 108, 111, 119, 101, 114)},\n",
       " (32, 119): {(32, 119, 105, 100, 101, 115, 116)},\n",
       " (119, 105): {(32, 119, 105, 100, 101, 115, 116)},\n",
       " (105, 100): {(32, 119, 105, 100, 101, 115, 116)},\n",
       " (100, 101): {(32, 119, 105, 100, 101, 115, 116)},\n",
       " (101, 115): {(32, 110, 101, 119, 101, 115, 116),\n",
       "  (32, 119, 105, 100, 101, 115, 116)},\n",
       " (115, 116): {(32, 110, 101, 119, 101, 115, 116),\n",
       "  (32, 119, 105, 100, 101, 115, 116)},\n",
       " (32, 110): {(32, 110, 101, 119, 101, 115, 116)},\n",
       " (110, 101): {(32, 110, 101, 119, 101, 115, 116)},\n",
       " (101, 119): {(32, 110, 101, 119, 101, 115, 116)}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_pretoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dab9a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a heap of (-cnt, -pair), negation is to maintain it as max heap (python defaults to min heap)\n",
    "# -pair is to maintain the same tie resolution as in our naive implementation\n",
    "heap = [(-cnt, (-p[0], -p[1])) for p, cnt in pairs_count.items()]\n",
    "heapq.heapify(heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "771c06c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9, (-115, -116)),\n",
       " (-9, (-101, -115)),\n",
       " (-7, (-111, -119)),\n",
       " (-8, (-119, -101)),\n",
       " (-7, (-108, -111)),\n",
       " (-6, (-110, -101)),\n",
       " (-6, (-101, -119)),\n",
       " (-3, (-105, -100)),\n",
       " (-3, (-100, -101)),\n",
       " (-7, (-32, -108)),\n",
       " (-2, (-101, -114)),\n",
       " (-6, (-32, -110)),\n",
       " (-3, (-32, -119)),\n",
       " (-3, (-119, -105))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e30334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index\n",
    "new_ix = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4a7f9",
   "metadata": {},
   "source": [
    "### Iteration Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35e3c306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 116) b's' b't'\n"
     ]
    }
   ],
   "source": [
    "# One merge iteration\n",
    "# Find most frequent pair. Note: the pair popped from the heap can be invalid (stale). \n",
    "# It can be so if we popping the not up-to-date (intermediary) pair count, pushed\n",
    "# during processing of overlapping pairs.\n",
    "# That's why we need to compare it with the actual state.\n",
    "while True:\n",
    "  neg_cnt, neg_p = heapq.heappop(heap)\n",
    "  top_pair = (-neg_p[0], -neg_p[1])\n",
    "  # if count is correct we are fine to break\n",
    "  # if top_pair already left pairs counter, get returns None\n",
    "  if pairs_count.get(top_pair) == -neg_cnt:\n",
    "    break\n",
    "  else: # invalid (stale) entry\n",
    "    continue\n",
    "print(top_pair, vocab[top_pair[0]], vocab[top_pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11ebce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9, (-101, -115)),\n",
       " (-8, (-119, -101)),\n",
       " (-7, (-111, -119)),\n",
       " (-3, (-119, -105)),\n",
       " (-7, (-108, -111)),\n",
       " (-6, (-110, -101)),\n",
       " (-6, (-101, -119)),\n",
       " (-3, (-105, -100)),\n",
       " (-3, (-100, -101)),\n",
       " (-7, (-32, -108)),\n",
       " (-2, (-101, -114)),\n",
       " (-6, (-32, -110)),\n",
       " (-3, (-32, -119))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2cabc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(seq, pair, new_ix):\n",
    "  if not isinstance(pair, tuple) or len(pair) != 2:\n",
    "    raise ValueError('`pair` must be a 2-tuple')\n",
    "  new_seq = []\n",
    "  out_pairs = []\n",
    "  in_pairs = []\n",
    "  i = 0\n",
    "  while i < len(seq):\n",
    "    # match branch\n",
    "    if i + 1 < len(seq) and (seq[i], seq[i + 1]) == pair:\n",
    "      new_seq.append(new_ix)\n",
    "      # if our pair is (x,y), we want to keep track of all (u,x) and (y,v) pairs\n",
    "      # as well as incoming (u, new_ix) and (new_ix, v)\n",
    "      # add outgoing and incoming pairs if any\n",
    "      if i - 1 >= 0:\n",
    "        # have (u,x) and (u, new_ix)\n",
    "        out_pairs.append((seq[i-1], seq[i]))\n",
    "        in_pairs.append((seq[i-1], new_ix))\n",
    "      if i + 2 < len(seq):\n",
    "        # have (y,v) and (new_ix, v)\n",
    "        out_pairs.append((seq[i+1], seq[i+2]))\n",
    "        in_pairs.append((new_ix, seq[i+2]))\n",
    "      i += 2  # correct step\n",
    "    else:\n",
    "      new_seq.append(seq[i])  # only current position\n",
    "      i += 1\n",
    "  return tuple(new_seq), out_pairs, in_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49d0e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pair_count(pair, freq):\n",
    "    c = pairs_count.get(pair)\n",
    "    if c is not None:  # guard\n",
    "      c += freq\n",
    "      if c > 0:  # update\n",
    "        pairs_count[pair] = c \n",
    "        # push it on heap to have up-to-date entry there\n",
    "        heapq.heappush(heap, (-c, (-pair[0], -pair[1])))\n",
    "      else:  # drop\n",
    "        pairs_count.pop(pair)\n",
    "\n",
    "def discard_pretoken(pt_seq, pair):\n",
    "    adj_set = pair_to_pretoken.get(pair)\n",
    "    if adj_set is not None:\n",
    "      adj_set.discard(pt_seq)\n",
    "      if adj_set:\n",
    "        pair_to_pretoken[pair] = adj_set\n",
    "      else:\n",
    "        pair_to_pretoken.pop(pair)\n",
    "\n",
    "def add_pretoken(pt_seq, pair):\n",
    "  adj_set = pair_to_pretoken.get(pair)\n",
    "  if adj_set is not None:\n",
    "    adj_set.add(pt_seq)\n",
    "  else:\n",
    "    adj_set = {pt_seq}\n",
    "  pair_to_pretoken[pair] = adj_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8301148",
   "metadata": {},
   "outputs": [],
   "source": [
    "if top_pair in pair_to_pretoken:\n",
    "  # now we can walk through only pretoken sequences actually containing the top pair\n",
    "  for pt in pair_to_pretoken[top_pair]:\n",
    "    freq = pretoken_count.get(pt) # current pretoken frequency\n",
    "    if freq is not None:\n",
    "      new_pt, out_pairs, in_pairs = merge(pt, top_pair, new_ix)\n",
    "      # update all our datastructures\n",
    "      for op in out_pairs:\n",
    "        update_pair_count(op, -freq)  # decrease count and update heap\n",
    "        # discard old pretoken from the set of pretokens containing outgoing pair\n",
    "        discard_pretoken(pt, op)\n",
    "      for ip in in_pairs:\n",
    "        # frequency of incoming pair has increased (+ update heap)\n",
    "        update_pair_count(ip, freq)\n",
    "        # add new pretoken to the set of pretokens containing incoming pair\n",
    "        add_pretoken(pt, ip)\n",
    "      # old pretoken pt is good to go\n",
    "      pretoken_count.pop(pt)\n",
    "      # new pretoken count update\n",
    "      pretoken_count[new_pt] = pretoken_count.get(new_pt, 0) + freq\n",
    "  # top pair doesn't exist anymore, so we need to clear its state\n",
    "  pairs_count.pop(top_pair)\n",
    "  pair_to_pretoken.pop(top_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36e1dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update merges and vocab\n",
    "merges.append(top_pair)\n",
    "vocab[new_ix] = vocab[top_pair[0]] + vocab[top_pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0e7474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increment next unit index\n",
    "new_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd79545f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(115, 116)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0bc4aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b' ', b'l', b'o', b'w'] -> 5\n",
      "[b' ', b'l', b'o', b'w', b'e', b'r'] -> 2\n",
      "[b' ', b'n', b'e', b'w', b'e', b'st'] -> 6\n",
      "[b' ', b'w', b'i', b'd', b'e', b'st'] -> 3\n"
     ]
    }
   ],
   "source": [
    "for pt, cnt in pretoken_count.items():\n",
    "  print([vocab[unit] for unit in pt], '->', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "282c323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 108) -> ( b' ' , b'l' ) -> 7\n",
      "(108, 111) -> ( b'l' , b'o' ) -> 7\n",
      "(111, 119) -> ( b'o' , b'w' ) -> 7\n",
      "(119, 101) -> ( b'w' , b'e' ) -> 8\n",
      "(101, 114) -> ( b'e' , b'r' ) -> 2\n",
      "(32, 119) -> ( b' ' , b'w' ) -> 3\n",
      "(119, 105) -> ( b'w' , b'i' ) -> 3\n",
      "(105, 100) -> ( b'i' , b'd' ) -> 3\n",
      "(100, 101) -> ( b'd' , b'e' ) -> 3\n",
      "(32, 110) -> ( b' ' , b'n' ) -> 6\n",
      "(110, 101) -> ( b'n' , b'e' ) -> 6\n",
      "(101, 119) -> ( b'e' , b'w' ) -> 6\n"
     ]
    }
   ],
   "source": [
    "for pair, cnt in pairs_count.items():\n",
    "  print(pair, '->', '(', vocab[pair[0]], ',', vocab[pair[1]], ')', '->', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a74181f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9, (-101, -115)),\n",
       " (-8, (-119, -101)),\n",
       " (-7, (-111, -119)),\n",
       " (-3, (-119, -105)),\n",
       " (-7, (-108, -111)),\n",
       " (-6, (-110, -101)),\n",
       " (-6, (-101, -119)),\n",
       " (-3, (-105, -100)),\n",
       " (-3, (-100, -101)),\n",
       " (-7, (-32, -108)),\n",
       " (-2, (-101, -114)),\n",
       " (-6, (-32, -110)),\n",
       " (-3, (-32, -119)),\n",
       " (-3, (-101, -115))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e6bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
